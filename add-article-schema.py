#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åšå®¢Article Schemaè‡ªåŠ¨æ·»åŠ è„šæœ¬
åŠŸèƒ½ï¼šä¸ºæ‰€æœ‰åšå®¢æ–‡ç« æ·»åŠ Articleç»“æ„åŒ–æ•°æ®
SEOæ•ˆæœï¼šæå‡æœç´¢ç»“æœå±•ç¤ºï¼Œå¯èƒ½å‡ºç°Rich Results
"""

import os
import re
import json
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup

def extract_article_info(html_content, file_path):
    """
    ä»HTMLä¸­æå–æ–‡ç« ä¿¡æ¯
    
    Returns:
        dict: æ–‡ç« ä¿¡æ¯
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå–title
    title_tag = soup.find('title')
    title = title_tag.string if title_tag else "VaultCaddy Blog Article"
    
    # æå–H1ä½œä¸ºheadline
    h1_tag = soup.find('h1')
    headline = h1_tag.get_text() if h1_tag else title
    
    # æå–meta description
    meta_desc = soup.find('meta', {'name': 'description'})
    description = meta_desc.get('content') if meta_desc else headline[:160]
    
    # æå–ç¬¬ä¸€ä¸ªå›¾ç‰‡ä½œä¸ºå°é¢
    first_img = soup.find('img')
    image_url = ""
    if first_img and first_img.get('src'):
        img_src = first_img.get('src')
        # è½¬æ¢ä¸ºç»å¯¹URL
        if img_src.startswith('http'):
            image_url = img_src
        elif img_src.startswith('/'):
            image_url = f"https://vaultcaddy.com{img_src}"
        else:
            # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦è®¡ç®—
            rel_path = os.path.dirname(file_path)
            image_url = f"https://vaultcaddy.com/{rel_path}/{img_src}"
    
    # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤OGå›¾ç‰‡
    if not image_url:
        image_url = "https://vaultcaddy.com/images/og-vaultcaddy-main.jpg"
    
    # æå–æˆ–ä¼°ç®—å­—æ•°
    body_text = soup.get_text()
    word_count = len(body_text.split())
    
    # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºå‘å¸ƒæ—¥æœŸ
    file_mtime = os.path.getmtime(file_path)
    date_published = datetime.fromtimestamp(file_mtime).isoformat()
    date_modified = date_published  # ç®€åŒ–å¤„ç†
    
    return {
        'headline': headline[:110],  # Googleé™åˆ¶110å­—ç¬¦
        'description': description[:160],
        'image': image_url,
        'datePublished': date_published,
        'dateModified': date_modified,
        'wordCount': word_count,
        'url': None  # éœ€è¦åç»­è®¾ç½®
    }

def create_article_schema(article_info):
    """
    åˆ›å»ºArticle Schema JSON-LD
    
    Args:
        article_info: æ–‡ç« ä¿¡æ¯å­—å…¸
    
    Returns:
        str: JSON-LDå­—ç¬¦ä¸²
    """
    schema = {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": article_info['headline'],
        "description": article_info['description'],
        "image": [article_info['image']],
        "datePublished": article_info['datePublished'],
        "dateModified": article_info['dateModified'],
        "author": {
            "@type": "Organization",
            "name": "VaultCaddy",
            "url": "https://vaultcaddy.com"
        },
        "publisher": {
            "@type": "Organization",
            "name": "VaultCaddy",
            "logo": {
                "@type": "ImageObject",
                "url": "https://vaultcaddy.com/images/vaultcaddy-logo.png"
            }
        },
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": article_info.get('url', 'https://vaultcaddy.com')
        },
        "wordCount": article_info['wordCount'],
        "articleBody": article_info['description']
    }
    
    return json.dumps(schema, ensure_ascii=False, indent=2)

def add_schema_to_html(html_content, schema_json):
    """
    å°†Schemaæ·»åŠ åˆ°HTMLçš„<head>ä¸­
    
    Args:
        html_content: åŸHTMLå†…å®¹
        schema_json: Schema JSONå­—ç¬¦ä¸²
    
    Returns:
        str: æ›´æ–°åçš„HTMLå†…å®¹
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰Article Schema
    existing_schemas = soup.find_all('script', {'type': 'application/ld+json'})
    for script in existing_schemas:
        try:
            schema_data = json.loads(script.string)
            if schema_data.get('@type') == 'Article':
                # å·²ç»æœ‰Article Schemaï¼Œä¸é‡å¤æ·»åŠ 
                return None
        except:
            pass
    
    # åˆ›å»ºæ–°çš„scriptæ ‡ç­¾
    new_script = soup.new_tag('script', type='application/ld+json')
    new_script.string = schema_json
    
    # æ·»åŠ åˆ°headä¸­
    head = soup.find('head')
    if head:
        head.append('\n    ')
        head.append(new_script)
        head.append('\n')
        return str(soup)
    else:
        return None

def process_blog_file(file_path, dry_run=False):
    """
    å¤„ç†å•ä¸ªåšå®¢æ–‡ä»¶
    
    Args:
        file_path: åšå®¢æ–‡ä»¶è·¯å¾„
        dry_run: æ˜¯å¦åªé¢„è§ˆ
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸæ·»åŠ Schema
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # æå–æ–‡ç« ä¿¡æ¯
        article_info = extract_article_info(html_content, file_path)
        
        # ç”ŸæˆURL
        rel_path = os.path.relpath(file_path, '.')
        article_info['url'] = f"https://vaultcaddy.com/{rel_path}"
        
        # åˆ›å»ºSchema
        schema_json = create_article_schema(article_info)
        
        # æ·»åŠ åˆ°HTML
        new_html = add_schema_to_html(html_content, schema_json)
        
        if new_html is None:
            # å·²ç»æœ‰Schemaæˆ–æ— æ³•æ·»åŠ 
            return False
        
        if not dry_run:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(new_html)
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥ {file_path}: {e}")
        return False

def find_blog_files(blog_dirs=['blog/', 'en/blog/', 'jp/blog/', 'kr/blog/']):
    """
    æŸ¥æ‰¾æ‰€æœ‰åšå®¢æ–‡ä»¶
    
    Args:
        blog_dirs: åšå®¢ç›®å½•åˆ—è¡¨
    
    Returns:
        list: åšå®¢æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    blog_files = []
    
    for blog_dir in blog_dirs:
        if not os.path.exists(blog_dir):
            continue
        
        for root, dirs, files in os.walk(blog_dir):
            for file in files:
                if file.endswith('.html') and file != 'index.html':
                    blog_files.append(os.path.join(root, file))
    
    return blog_files

def batch_add_article_schema(dry_run=False):
    """
    æ‰¹é‡ä¸ºåšå®¢æ–‡ç« æ·»åŠ Article Schema
    
    Args:
        dry_run: æ˜¯å¦åªé¢„è§ˆ
    """
    print("ğŸ“ åšå®¢Article Schemaæ·»åŠ å·¥å…·")
    print("=" * 60)
    print(f"ğŸ§ª é¢„è§ˆæ¨¡å¼: {'æ˜¯' if dry_run else 'å¦'}")
    print("-" * 60)
    
    # æŸ¥æ‰¾æ‰€æœ‰åšå®¢æ–‡ä»¶
    blog_files = find_blog_files()
    print(f"ğŸ“Š æ‰¾åˆ° {len(blog_files)} ä¸ªåšå®¢æ–‡ä»¶\n")
    
    if not blog_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•åšå®¢æ–‡ä»¶")
        return
    
    success_count = 0
    skipped_count = 0
    error_count = 0
    
    for i, file_path in enumerate(blog_files, 1):
        print(f"ğŸ”„ [{i}/{len(blog_files)}] å¤„ç† {os.path.relpath(file_path)}...", end=' ')
        
        result = process_blog_file(file_path, dry_run=dry_run)
        
        if result:
            success_count += 1
            status = "(é¢„è§ˆ)" if dry_run else "âœ…"
            print(f"{status} å·²æ·»åŠ Article Schema")
        else:
            skipped_count += 1
            print("â­ï¸  å·²æœ‰Schemaï¼Œè·³è¿‡")
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æ·»åŠ å®Œæˆæ€»ç»“")
    print("=" * 60)
    print(f"ğŸ“ æ‰«ææ–‡ä»¶: {len(blog_files)} ä¸ª")
    print(f"âœ… æˆåŠŸæ·»åŠ : {success_count} ä¸ª")
    print(f"â­ï¸  è·³è¿‡: {skipped_count} ä¸ª")
    print(f"âŒ å¤±è´¥: {error_count} ä¸ª")
    
    if success_count > 0:
        print(f"\nğŸš€ é¢„æœŸSEOæ•ˆæœ:")
        print(f"   âœ… æœç´¢ç»“æœå¯èƒ½æ˜¾ç¤ºå‘å¸ƒæ—¥æœŸã€ä½œè€…ä¿¡æ¯")
        print(f"   âœ… æå‡ç‚¹å‡»ç‡ (CTR) +10-20%")
        print(f"   âœ… Googleå¯èƒ½å±•ç¤ºRich Results")
        print(f"   âœ… æ›´å¥½çš„æ–‡ç« å¯è§åº¦")
        
        print(f"\nğŸ§ª éªŒè¯æ–¹æ³•:")
        print(f"   1. è®¿é—®: https://search.google.com/test/rich-results")
        print(f"   2. è¾“å…¥åšå®¢æ–‡ç« URL")
        print(f"   3. æŸ¥çœ‹æ˜¯å¦è¯†åˆ«ä¸ºArticle")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡ä¸ºåšå®¢æ–‡ç« æ·»åŠ Article Schema')
    parser.add_argument('-d', '--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹ï¼‰')
    
    args = parser.parse_args()
    
    batch_add_article_schema(dry_run=args.dry_run)

if __name__ == '__main__':
    main()

