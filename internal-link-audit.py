#!/usr/bin/env python3
"""
å†…éƒ¨é“¾æ¥å®¡è®¡è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰«ææ‰€æœ‰HTMLæ–‡ä»¶
2. æå–æ‰€æœ‰å†…éƒ¨é“¾æ¥
3. æ£€æµ‹æ–­é“¾ï¼ˆ404ï¼‰
4. è¯†åˆ«å­¤ç«‹é¡µé¢ï¼ˆå…¥ç«™é“¾æ¥=0ï¼‰
5. è®¡ç®—é“¾æ¥æ·±åº¦
6. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
python3 internal-link-audit.py
"""

import os
import re
from pathlib import Path
from bs4 import BeautifulSoup
from collections import defaultdict
import csv

class InternalLinkAuditor:
    def __init__(self, root_dir='.'):
        self.root_dir = root_dir
        self.all_pages = set()
        self.links = defaultdict(list)  # page -> list of links
        self.incoming_links = defaultdict(list)  # page -> list of referring pages
        self.broken_links = []
        self.orphan_pages = []
        
    def find_all_html_files(self):
        """æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶"""
        html_files = []
        
        # æ’é™¤ç›®å½•
        exclude_dirs = {'node_modules', '.git', 'terminals', '__pycache__', '.vscode'}
        
        for root, dirs, files in os.walk(self.root_dir):
            # ç§»é™¤æ’é™¤çš„ç›®å½•
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                if file.endswith('.html'):
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, self.root_dir)
                    html_files.append(rel_path)
                    self.all_pages.add(rel_path)
        
        return html_files
    
    def normalize_link(self, link, current_page):
        """è§„èŒƒåŒ–é“¾æ¥è·¯å¾„"""
        # ç§»é™¤é”šç‚¹
        if '#' in link:
            link = link.split('#')[0]
        
        # ç§»é™¤æŸ¥è¯¢å‚æ•°
        if '?' in link:
            link = link.split('?')[0]
        
        # å¦‚æœé“¾æ¥ä¸ºç©ºï¼Œè¿”å›None
        if not link or link == '':
            return None
        
        # ç»å¯¹URLï¼Œå¿½ç•¥
        if link.startswith('http://') or link.startswith('https://'):
            return None
        
        # ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if link.startswith('/'):
            # ä»æ ¹ç›®å½•å¼€å§‹
            link = link[1:]
        else:
            # ç›¸å¯¹äºå½“å‰é¡µé¢
            current_dir = os.path.dirname(current_page)
            link = os.path.normpath(os.path.join(current_dir, link))
        
        return link
    
    def extract_links(self, file_path):
        """æå–é¡µé¢ä¸­çš„æ‰€æœ‰å†…éƒ¨é“¾æ¥"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            links = []
            
            # æå–æ‰€æœ‰<a>æ ‡ç­¾çš„href
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                normalized = self.normalize_link(href, os.path.relpath(file_path, self.root_dir))
                if normalized:
                    links.append(normalized)
            
            return links
        except Exception as e:
            print(f"  âŒ é”™è¯¯å¤„ç† {file_path}: {e}")
            return []
    
    def check_broken_links(self):
        """æ£€æŸ¥æ–­é“¾"""
        for page, links in self.links.items():
            for link in links:
                link_path = os.path.join(self.root_dir, link)
                if not os.path.exists(link_path):
                    self.broken_links.append({
                        'source': page,
                        'broken_link': link
                    })
    
    def find_orphan_pages(self):
        """æ‰¾å‡ºå­¤ç«‹é¡µé¢ï¼ˆæ²¡æœ‰å…¥ç«™é“¾æ¥ï¼‰"""
        # æ’é™¤ç‰¹æ®Šé¡µé¢
        exclude_pages = {'404.html', 'index.html', 'en/index.html', 'jp/index.html', 'kr/index.html'}
        
        for page in self.all_pages:
            if page not in exclude_pages:
                if len(self.incoming_links[page]) == 0:
                    self.orphan_pages.append(page)
    
    def calculate_link_depth(self, start_page='index.html', max_depth=10):
        """è®¡ç®—é¡µé¢é“¾æ¥æ·±åº¦ï¼ˆä»é¦–é¡µå¼€å§‹ï¼‰"""
        depths = {start_page: 0}
        queue = [(start_page, 0)]
        visited = set()
        
        while queue:
            current_page, depth = queue.pop(0)
            
            if current_page in visited or depth >= max_depth:
                continue
            
            visited.add(current_page)
            
            # éå†å½“å‰é¡µé¢çš„é“¾æ¥
            for link in self.links.get(current_page, []):
                if link not in depths:
                    depths[link] = depth + 1
                    queue.append((link, depth + 1))
        
        return depths
    
    def generate_report(self):
        """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_pages = len(self.all_pages)
        total_internal_links = sum(len(links) for links in self.links.values())
        avg_links_per_page = total_internal_links / total_pages if total_pages > 0 else 0
        
        # é“¾æ¥æ·±åº¦
        depths = self.calculate_link_depth()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š å†…éƒ¨é“¾æ¥å®¡è®¡æŠ¥å‘Š")
        print("=" * 60)
        
        print(f"\nâœ… åŸºæœ¬ç»Ÿè®¡ï¼š")
        print(f"  - æ€»é¡µé¢æ•°ï¼š{total_pages}")
        print(f"  - æ€»å†…éƒ¨é“¾æ¥æ•°ï¼š{total_internal_links}")
        print(f"  - å¹³å‡æ¯é¡µé“¾æ¥æ•°ï¼š{avg_links_per_page:.1f}")
        
        print(f"\nâŒ é—®é¢˜å‘ç°ï¼š")
        print(f"  - æ–­é“¾æ•°é‡ï¼š{len(self.broken_links)}")
        print(f"  - å­¤ç«‹é¡µé¢ï¼š{len(self.orphan_pages)}")
        
        if self.broken_links:
            print(f"\nğŸ”´ æ–­é“¾è¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰ï¼š")
            for i, broken in enumerate(self.broken_links[:10], 1):
                print(f"  {i}. {broken['source']} â†’ {broken['broken_link']}")
            if len(self.broken_links) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.broken_links) - 10} ä¸ªæ–­é“¾")
        
        if self.orphan_pages:
            print(f"\nğŸŸ¡ å­¤ç«‹é¡µé¢ï¼ˆå‰10ä¸ªï¼‰ï¼š")
            for i, orphan in enumerate(self.orphan_pages[:10], 1):
                print(f"  {i}. {orphan}")
            if len(self.orphan_pages) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.orphan_pages) - 10} ä¸ªå­¤ç«‹é¡µé¢")
        
        # é“¾æ¥æ·±åº¦åˆ†æ
        print(f"\nğŸ“ é“¾æ¥æ·±åº¦åˆ†æï¼š")
        depth_counts = defaultdict(int)
        for page, depth in depths.items():
            depth_counts[depth] += 1
        
        for depth in sorted(depth_counts.keys()):
            print(f"  - æ·±åº¦ {depth}ï¼š{depth_counts[depth]} é¡µé¢")
        
        # å…¥ç«™é“¾æ¥æœ€å¤šçš„é¡µé¢
        print(f"\nğŸ”— å…¥ç«™é“¾æ¥æœ€å¤šçš„é¡µé¢ï¼ˆTop 10ï¼‰ï¼š")
        top_pages = sorted(self.incoming_links.items(), key=lambda x: len(x[1]), reverse=True)[:10]
        for i, (page, referring_pages) in enumerate(top_pages, 1):
            print(f"  {i}. {page}: {len(referring_pages)} å…¥ç«™é“¾æ¥")
        
        # å‡ºç«™é“¾æ¥æœ€å¤šçš„é¡µé¢
        print(f"\nğŸ”— å‡ºç«™é“¾æ¥æœ€å¤šçš„é¡µé¢ï¼ˆTop 10ï¼‰ï¼š")
        top_pages = sorted(self.links.items(), key=lambda x: len(x[1]), reverse=True)[:10]
        for i, (page, links) in enumerate(top_pages, 1):
            print(f"  {i}. {page}: {len(links)} å‡ºç«™é“¾æ¥")
        
        return {
            'total_pages': total_pages,
            'total_links': total_internal_links,
            'broken_links': len(self.broken_links),
            'orphan_pages': len(self.orphan_pages)
        }
    
    def export_to_csv(self):
        """å¯¼å‡ºè¯¦ç»†æŠ¥å‘Šåˆ°CSV"""
        # å¯¼å‡ºé“¾æ¥è¯¦æƒ…
        with open('internal-links-report.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['æºé¡µé¢', 'ç›®æ ‡é“¾æ¥', 'å…¥ç«™é“¾æ¥æ•°'])
            
            for page, links in sorted(self.links.items()):
                for link in links:
                    incoming_count = len(self.incoming_links.get(link, []))
                    writer.writerow([page, link, incoming_count])
        
        print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²å¯¼å‡ºåˆ°ï¼šinternal-links-report.csv")
        
        # å¯¼å‡ºæ–­é“¾
        if self.broken_links:
            with open('broken-links.csv', 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['æºé¡µé¢', 'æ–­é“¾'])
                for broken in self.broken_links:
                    writer.writerow([broken['source'], broken['broken_link']])
            print(f"âœ… æ–­é“¾æŠ¥å‘Šå·²å¯¼å‡ºåˆ°ï¼šbroken-links.csv")
        
        # å¯¼å‡ºå­¤ç«‹é¡µé¢
        if self.orphan_pages:
            with open('orphan-pages.txt', 'w', encoding='utf-8') as f:
                for orphan in self.orphan_pages:
                    f.write(orphan + '\n')
            print(f"âœ… å­¤ç«‹é¡µé¢åˆ—è¡¨å·²å¯¼å‡ºåˆ°ï¼šorphan-pages.txt")
    
    def run(self):
        """è¿è¡Œå®¡è®¡"""
        print("ğŸš€ å¼€å§‹å†…éƒ¨é“¾æ¥å®¡è®¡...")
        print("=" * 60)
        
        # 1. æ‰¾åˆ°æ‰€æœ‰HTMLæ–‡ä»¶
        print(f"\nğŸ“ æ‰«æHTMLæ–‡ä»¶...")
        html_files = self.find_all_html_files()
        print(f"  æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        
        # 2. æå–æ‰€æœ‰é“¾æ¥
        print(f"\nğŸ” æå–å†…éƒ¨é“¾æ¥...")
        for html_file in html_files:
            file_path = os.path.join(self.root_dir, html_file)
            links = self.extract_links(file_path)
            self.links[html_file] = links
            
            # è®°å½•å…¥ç«™é“¾æ¥
            for link in links:
                self.incoming_links[link].append(html_file)
        
        print(f"  æå–äº† {sum(len(links) for links in self.links.values())} ä¸ªå†…éƒ¨é“¾æ¥")
        
        # 3. æ£€æŸ¥æ–­é“¾
        print(f"\nğŸ” æ£€æŸ¥æ–­é“¾...")
        self.check_broken_links()
        print(f"  å‘ç° {len(self.broken_links)} ä¸ªæ–­é“¾")
        
        # 4. æ‰¾å‡ºå­¤ç«‹é¡µé¢
        print(f"\nğŸ” æŸ¥æ‰¾å­¤ç«‹é¡µé¢...")
        self.find_orphan_pages()
        print(f"  å‘ç° {len(self.orphan_pages)} ä¸ªå­¤ç«‹é¡µé¢")
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        # 6. å¯¼å‡ºCSV
        self.export_to_csv()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ å®¡è®¡å®Œæˆï¼")
        print("=" * 60)

def main():
    auditor = InternalLinkAuditor()
    auditor.run()

if __name__ == '__main__':
    main()

