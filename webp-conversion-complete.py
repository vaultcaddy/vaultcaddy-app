#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WebPå®Œæ•´è½¬æ¢å’ŒHTMLæ›´æ–°æ–¹æ¡ˆ
åŒ…å«ï¼šå›¾ç‰‡è½¬æ¢ + HTMLè‡ªåŠ¨æ›´æ–° + éªŒè¯
"""

import os
import re
from pathlib import Path
from PIL import Image
from bs4 import BeautifulSoup
import concurrent.futures

class WebPConverter:
    def __init__(self, base_dir='.', quality=80, keep_original=True):
        self.base_dir = base_dir
        self.quality = quality
        self.keep_original = keep_original
        self.converted_count = 0
        self.failed_files = []
        self.total_saved_bytes = 0
        
    def convert_image(self, image_path):
        """è½¬æ¢å•ä¸ªå›¾ç‰‡ä¸ºWebP"""
        try:
            # æ‰“å¼€å›¾ç‰‡
            with Image.open(image_path) as img:
                # ç”Ÿæˆè¾“å‡ºè·¯å¾„
                output_path = str(Path(image_path).with_suffix('.webp'))
                
                # å¦‚æœWebPå·²å­˜åœ¨ä¸”æ¯”åŸå›¾æ–°ï¼Œè·³è¿‡
                if os.path.exists(output_path):
                    if os.path.getmtime(output_path) > os.path.getmtime(image_path):
                        return True, f"å·²å­˜åœ¨: {output_path}"
                
                # ä¿å­˜ä¸ºWebPï¼ˆä¿ç•™é€æ˜åº¦ï¼‰
                if img.mode in ('RGBA', 'LA'):
                    img.save(output_path, 'WEBP', quality=self.quality, method=6, lossless=False)
                else:
                    # è½¬æ¢ä¸ºRGB
                    rgb_img = img.convert('RGB')
                    rgb_img.save(output_path, 'WEBP', quality=self.quality, method=6)
                
                # è®¡ç®—æ–‡ä»¶å¤§å°èŠ‚çœ
                original_size = os.path.getsize(image_path)
                webp_size = os.path.getsize(output_path)
                saved_bytes = original_size - webp_size
                
                self.converted_count += 1
                self.total_saved_bytes += saved_bytes
                
                reduction_pct = (saved_bytes / original_size) * 100
                
                return True, f"è½¬æ¢æˆåŠŸ: {image_path} â†’ {output_path} (å‡å°‘ {reduction_pct:.1f}%)"
                
        except Exception as e:
            self.failed_files.append((image_path, str(e)))
            return False, f"è½¬æ¢å¤±è´¥: {image_path} - {e}"
    
    def find_images(self):
        """æŸ¥æ‰¾æ‰€æœ‰éœ€è¦è½¬æ¢çš„å›¾ç‰‡"""
        image_extensions = {'.jpg', '.jpeg', '.png'}
        ignore_dirs = {'node_modules', '.git', 'venv', '__pycache__'}
        
        images = []
        for root, dirs, files in os.walk(self.base_dir):
            # è¿‡æ»¤å¿½ç•¥çš„ç›®å½•
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            
            for file in files:
                if Path(file).suffix.lower() in image_extensions:
                    full_path = os.path.join(root, file)
                    images.append(full_path)
        
        return images
    
    def batch_convert(self, max_workers=4):
        """æ‰¹é‡è½¬æ¢å›¾ç‰‡"""
        images = self.find_images()
        
        if not images:
            print("  âš ï¸  æœªæ‰¾åˆ°éœ€è¦è½¬æ¢çš„å›¾ç‰‡")
            return
        
        print(f"\nğŸ“¸ æ‰¾åˆ° {len(images)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡è½¬æ¢ï¼ˆä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹ï¼‰...\n")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(self.convert_image, img) for img in images]
            
            for future in concurrent.futures.as_completed(futures):
                success, message = future.result()
                if success:
                    print(f"  âœ… {message}")
                else:
                    print(f"  âŒ {message}")
    
    def update_html_file(self, html_path):
        """æ›´æ–°HTMLæ–‡ä»¶ä½¿ç”¨WebP"""
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            modified = False
            updates = 0
            
            for img in soup.find_all('img'):
                src = img.get('src')
                if not src:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°å›¾ç‰‡
                if src.startswith('http'):
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯webp
                if src.endswith('.webp'):
                    continue
                
                # æ£€æŸ¥å¯¹åº”çš„webpæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                img_path = Path(src)
                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    webp_src = str(img_path.with_suffix('.webp'))
                    
                    # è®¡ç®—ç›¸å¯¹äºHTMLæ–‡ä»¶çš„è·¯å¾„
                    html_dir = Path(html_path).parent
                    webp_full_path = html_dir / webp_src
                    
                    if webp_full_path.exists():
                        # åˆ›å»ºpictureæ ‡ç­¾ä»¥æ”¯æŒfallback
                        picture = soup.new_tag('picture')
                        
                        # WebP source
                        source_webp = soup.new_tag('source', srcset=webp_src, type='image/webp')
                        picture.append(source_webp)
                        
                        # åŸå§‹æ ¼å¼source
                        source_original = soup.new_tag('source', srcset=src, type=f'image/{img_path.suffix[1:]}')
                        picture.append(source_original)
                        
                        # å¤åˆ¶imgçš„æ‰€æœ‰å±æ€§
                        new_img = soup.new_tag('img')
                        for attr, value in img.attrs.items():
                            new_img[attr] = value
                        
                        picture.append(new_img)
                        
                        # æ›¿æ¢åŸimgæ ‡ç­¾
                        img.replace_with(picture)
                        
                        updates += 1
                        modified = True
            
            if modified:
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                return True, f"æ›´æ–° {updates} ä¸ªå›¾ç‰‡æ ‡ç­¾"
            
            return False, "æ— éœ€æ›´æ–°"
            
        except Exception as e:
            return False, f"æ›´æ–°å¤±è´¥: {e}"
    
    def update_all_html(self):
        """æ›´æ–°æ‰€æœ‰HTMLæ–‡ä»¶"""
        html_files = []
        ignore_dirs = {'node_modules', '.git', 'venv', '__pycache__', 'terminals'}
        
        for root, dirs, files in os.walk(self.base_dir):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))
        
        if not html_files:
            print("  âš ï¸  æœªæ‰¾åˆ°HTMLæ–‡ä»¶")
            return
        
        print(f"\nğŸ“„ æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        print(f"ğŸ”„ å¼€å§‹æ›´æ–°...\n")
        
        updated_count = 0
        for html_file in html_files:
            success, message = self.update_html_file(html_file)
            if success:
                updated_count += 1
                print(f"  âœ… {html_file}: {message}")
            else:
                print(f"  â­ï¸  {html_file}: {message}")
        
        return updated_count
    
    def generate_report(self):
        """ç”Ÿæˆè½¬æ¢æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š WebPè½¬æ¢å®ŒæˆæŠ¥å‘Š")
        print("=" * 70)
        
        print(f"\nâœ… æˆåŠŸè½¬æ¢: {self.converted_count} ä¸ªå›¾ç‰‡")
        print(f"âŒ è½¬æ¢å¤±è´¥: {len(self.failed_files)} ä¸ªå›¾ç‰‡")
        
        if self.total_saved_bytes > 0:
            mb_saved = self.total_saved_bytes / (1024 * 1024)
            print(f"ğŸ’¾ èŠ‚çœç©ºé—´: {mb_saved:.2f} MB")
            
            if self.converted_count > 0:
                avg_reduction = (self.total_saved_bytes / self.converted_count) / 1024
                print(f"ğŸ“‰ å¹³å‡å‡å°‘: {avg_reduction:.1f} KB/å›¾ç‰‡")
        
        if self.failed_files:
            print(f"\nâš ï¸  å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
            for file, error in self.failed_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {file}: {error}")
            if len(self.failed_files) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.failed_files) - 10} ä¸ªå¤±è´¥")
        
        print(f"\nğŸ¯ é¢„æœŸæ•ˆæœ:")
        print(f"  âœ… é¡µé¢åŠ è½½é€Ÿåº¦æå‡: 30-50%")
        print(f"  âœ… LCP (Largest Contentful Paint): é™ä½30-40%")
        print(f"  âœ… å¸¦å®½æ¶ˆè€—å‡å°‘: 40-60%")
        print(f"  âœ… PageSpeed Score: æå‡10-15åˆ†")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"  1. æµ‹è¯•ç½‘ç«™æ‰€æœ‰å›¾ç‰‡æ˜¯å¦æ­£å¸¸æ˜¾ç¤º")
        print(f"  2. ä½¿ç”¨ PageSpeed Insights éªŒè¯æ•ˆæœ")
        print(f"  3. åœ¨ä¸åŒæµè§ˆå™¨æµ‹è¯•å…¼å®¹æ€§")
        print(f"  4. å¦‚æœ‰é—®é¢˜ï¼Œå¯å›æ»šåˆ°åŸå§‹å›¾ç‰‡")

def main():
    print("ğŸ–¼ï¸  WebP å®Œæ•´è½¬æ¢æ–¹æ¡ˆ")
    print("=" * 70)
    print("åŠŸèƒ½: å›¾ç‰‡è½¬æ¢ + HTMLè‡ªåŠ¨æ›´æ–° + å…¼å®¹æ€§å¤„ç†")
    print("-" * 70)
    
    # åˆ›å»ºè½¬æ¢å™¨
    converter = WebPConverter(base_dir='.', quality=80, keep_original=True)
    
    # æ­¥éª¤1: è½¬æ¢å›¾ç‰‡
    print("\nğŸ“‹ æ­¥éª¤1: æ‰¹é‡è½¬æ¢å›¾ç‰‡ä¸ºWebPæ ¼å¼")
    print("-" * 70)
    converter.batch_convert(max_workers=4)
    
    # æ­¥éª¤2: æ›´æ–°HTML
    print("\nğŸ“‹ æ­¥éª¤2: æ›´æ–°HTMLæ–‡ä»¶ä½¿ç”¨WebP")
    print("-" * 70)
    updated_count = converter.update_all_html()
    
    # æ­¥éª¤3: ç”ŸæˆæŠ¥å‘Š
    converter.generate_report()
    
    if updated_count:
        print(f"\nğŸŠ HTMLæ›´æ–°: {updated_count} ä¸ªæ–‡ä»¶å·²æ›´æ–°")

if __name__ == '__main__':
    main()

