#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ç”ŸæˆSitemap for VaultCaddy
åŒ…æ‹¬æ‰€æœ‰é¡µé¢ï¼šv3, v2, simple, åŠŸèƒ½é¡µé¢, åšå®¢æ–‡ç« ç­‰
"""

import os
from pathlib import Path
from datetime import datetime
import xml.etree.ElementTree as ET
from xml.dom import minidom

class SitemapGenerator:
    """Sitemapç”Ÿæˆå™¨"""
    
    def __init__(self, root_dir, base_url):
        self.root_dir = Path(root_dir)
        self.base_url = base_url.rstrip('/')
        self.pages = []
        
        # é¡µé¢ä¼˜å…ˆçº§é…ç½®
        self.priority_map = {
            'index.html': 1.0,
            'pricing.html': 0.9,
            'signup.html': 0.9,
            'login.html': 0.8,
            'v3': 0.9,  # v3é“¶è¡Œé¡µé¢
            'v2': 0.8,  # v2é¡µé¢
            'simple': 0.7,  # simpleé¡µé¢
            'blog': 0.8,  # åšå®¢æ–‡ç« 
            'solution': 0.7,  # è§£å†³æ–¹æ¡ˆé¡µé¢
            'vs': 0.6,  # å¯¹æ¯”é¡µé¢
            'default': 0.5
        }
        
        # æ›´æ–°é¢‘ç‡é…ç½®
        self.changefreq_map = {
            'index.html': 'daily',
            'pricing.html': 'weekly',
            'blog': 'weekly',
            'v3': 'weekly',
            'v2': 'monthly',
            'simple': 'monthly',
            'default': 'monthly'
        }
    
    def get_priority(self, filepath):
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šä¼˜å…ˆçº§"""
        filename = filepath.name
        
        if filename in self.priority_map:
            return self.priority_map[filename]
        
        if '-v3.html' in filename:
            return self.priority_map['v3']
        elif '-v2.html' in filename:
            return self.priority_map['v2']
        elif '-simple.html' in filename:
            return self.priority_map['simple']
        elif 'blog/' in str(filepath):
            return self.priority_map['blog']
        elif '-solution' in filename or '-accounting' in filename:
            return self.priority_map['solution']
        elif '-vs-' in filename:
            return self.priority_map['vs']
        
        return self.priority_map['default']
    
    def get_changefreq(self, filepath):
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šæ›´æ–°é¢‘ç‡"""
        filename = filepath.name
        
        if filename in self.changefreq_map:
            return self.changefreq_map[filename]
        
        if '-v3.html' in filename:
            return self.changefreq_map['v3']
        elif '-v2.html' in filename:
            return self.changefreq_map['v2']
        elif '-simple.html' in filename:
            return self.changefreq_map['simple']
        elif 'blog/' in str(filepath):
            return self.changefreq_map['blog']
        
        return self.changefreq_map['default']
    
    def get_lastmod(self, filepath):
        """è·å–æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´"""
        try:
            mtime = filepath.stat().st_mtime
            return datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
        except:
            return datetime.now().strftime('%Y-%m-%d')
    
    def collect_pages(self):
        """æ”¶é›†æ‰€æœ‰HTMLé¡µé¢"""
        print("ğŸ” å¼€å§‹æ”¶é›†é¡µé¢...")
        
        # æ ¹ç›®å½•çš„ä¸»è¦é¡µé¢
        main_pages = ['index.html', 'pricing.html', 'signup.html', 'login.html', 
                      'privacy.html', 'terms.html', 'about.html', 'contact.html']
        
        for page in main_pages:
            page_file = self.root_dir / page
            if page_file.exists():
                self.pages.append(page_file)
        
        # æ‰€æœ‰landing pages
        for file in self.root_dir.glob('*.html'):
            if file.name not in main_pages and not file.name.startswith('.'):
                # æ’é™¤æµ‹è¯•é¡µé¢å’Œæ¨¡æ¿
                if not any(x in file.name for x in ['test-', 'template', 'backup', 'old', 'è¨ºæ–·']):
                    self.pages.append(file)
        
        # å­ç›®å½•ä¸­çš„é¡µé¢ï¼ˆå¤šè¯­è¨€ç‰ˆæœ¬ï¼‰
        for lang_dir in ['en', 'kr', 'jp', 'zh-HK', 'zh-TW', 'ja-JP', 'ko-KR']:
            lang_path = self.root_dir / lang_dir
            if lang_path.exists():
                for file in lang_path.glob('*.html'):
                    if not any(x in file.name for x in ['test-', 'template', 'backup', 'old']):
                        self.pages.append(file)
        
        # Blogç›®å½•
        blog_path = self.root_dir / 'blog'
        if blog_path.exists():
            for file in blog_path.glob('**/*.html'):
                self.pages.append(file)
        
        print(f"âœ… æ‰¾åˆ° {len(self.pages)} ä¸ªé¡µé¢")
    
    def generate_sitemap(self):
        """ç”Ÿæˆsitemap.xml"""
        print("\nğŸš€ å¼€å§‹ç”Ÿæˆ Sitemap...")
        
        # åˆ›å»ºXMLæ ¹å…ƒç´ 
        urlset = ET.Element('urlset')
        urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
        urlset.set('xmlns:xsi', 'http://www.w3.org/2001/XMLSchema-instance')
        urlset.set('xsi:schemaLocation', 'http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd')
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_pages = sorted(self.pages, key=lambda x: self.get_priority(x), reverse=True)
        
        for page in sorted_pages:
            # æ„å»ºURL
            rel_path = page.relative_to(self.root_dir)
            url_path = str(rel_path).replace('\\', '/')
            full_url = f"{self.base_url}/{url_path}"
            
            # åˆ›å»ºURLå…ƒç´ 
            url_elem = ET.SubElement(urlset, 'url')
            
            loc = ET.SubElement(url_elem, 'loc')
            loc.text = full_url
            
            lastmod = ET.SubElement(url_elem, 'lastmod')
            lastmod.text = self.get_lastmod(page)
            
            changefreq = ET.SubElement(url_elem, 'changefreq')
            changefreq.text = self.get_changefreq(page)
            
            priority = ET.SubElement(url_elem, 'priority')
            priority.text = str(self.get_priority(page))
        
        # æ ¼å¼åŒ–XML
        xml_string = ET.tostring(urlset, encoding='utf-8')
        dom = minidom.parseString(xml_string)
        pretty_xml = dom.toprettyxml(indent='  ', encoding='utf-8')
        
        # ç§»é™¤ç©ºè¡Œ
        lines = [line for line in pretty_xml.decode('utf-8').split('\n') if line.strip()]
        pretty_xml = '\n'.join(lines)
        
        # ä¿å­˜sitemap
        sitemap_file = self.root_dir / 'sitemap.xml'
        with open(sitemap_file, 'w', encoding='utf-8') as f:
            f.write(pretty_xml)
        
        print(f"âœ… Sitemapå·²ç”Ÿæˆ: {sitemap_file}")
        print(f"ğŸ“Š åŒ…å« {len(sorted_pages)} ä¸ªURL")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(sorted_pages)
        
        return sitemap_file
    
    def generate_robots_txt(self):
        """ç”Ÿæˆrobots.txt"""
        print("\nğŸ¤– ç”Ÿæˆ robots.txt...")
        
        robots_content = f"""# VaultCaddy Robots.txt
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

User-agent: *
Allow: /
Disallow: /admin/
Disallow: /api/
Disallow: /backup_*/
Disallow: /*test*.html
Disallow: /*backup*.html
Disallow: /*old*.html
Disallow: /*template*.html

# Sitemaps
Sitemap: {self.base_url}/sitemap.xml

# å¸¸è§æœç´¢å¼•æ“
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

# é™åˆ¶çˆ¬å–é¢‘ç‡
Crawl-delay: 1
"""
        
        robots_file = self.root_dir / 'robots.txt'
        with open(robots_file, 'w', encoding='utf-8') as f:
            f.write(robots_content)
        
        print(f"âœ… robots.txtå·²ç”Ÿæˆ: {robots_file}")
    
    def generate_report(self, pages):
        """ç”ŸæˆSitemapæŠ¥å‘Š"""
        report = f"""
# âœ… Sitemapç”Ÿæˆå®ŒæˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ç½‘ç«™**: {self.base_url}

---

## ğŸ“Š Sitemapç»Ÿè®¡

| æŒ‡æ ‡ | æ•°é‡ |
|------|------|
| **æ€»URLæ•°** | {len(pages)} |
| **ä¼˜å…ˆçº§1.0** | {len([p for p in pages if self.get_priority(p) == 1.0])} |
| **ä¼˜å…ˆçº§0.9** | {len([p for p in pages if self.get_priority(p) == 0.9])} |
| **ä¼˜å…ˆçº§0.8** | {len([p for p in pages if self.get_priority(p) == 0.8])} |
| **ä¼˜å…ˆçº§0.7** | {len([p for p in pages if self.get_priority(p) == 0.7])} |
| **ä¼˜å…ˆçº§â‰¤0.6** | {len([p for p in pages if self.get_priority(p) <= 0.6])} |

---

## ğŸ“‚ é¡µé¢åˆ†ç±»ç»Ÿè®¡

| ç±»å‹ | æ•°é‡ | ç¤ºä¾‹ |
|------|------|------|
| **ä¸»é¡µ** | {len([p for p in pages if p.name == 'index.html'])} | index.html |
| **v3é¡µé¢** | {len([p for p in pages if '-v3.html' in p.name])} | chase-bank-statement-v3.html |
| **v2é¡µé¢** | {len([p for p in pages if '-v2.html' in p.name])} | dz-bank-statement-v2.html |
| **simpleé¡µé¢** | {len([p for p in pages if '-simple.html' in p.name])} | smbc-bank-statement-simple.html |
| **è§£å†³æ–¹æ¡ˆ** | {len([p for p in pages if '-solution' in p.name or '-accounting' in p.name])} | restaurant-accounting-solution.html |
| **å¯¹æ¯”é¡µé¢** | {len([p for p in pages if '-vs-' in p.name])} | vaultcaddy-vs-nanonets.html |
| **åšå®¢æ–‡ç« ** | {len([p for p in pages if 'blog/' in str(p)])} | blog/*.html |
| **å…¶ä»–** | {len([p for p in pages if not any(x in p.name for x in ['-v3', '-v2', '-simple', '-solution', '-accounting', '-vs-', 'blog/'])])} | - |

---

## ğŸ¯ ä¼˜å…ˆçº§é¡µé¢åˆ—è¡¨ (Top 20)

"""
        
        # æ·»åŠ å‰20ä¸ªé«˜ä¼˜å…ˆçº§é¡µé¢
        top_pages = sorted(pages, key=lambda x: self.get_priority(x), reverse=True)[:20]
        
        for i, page in enumerate(top_pages, 1):
            priority = self.get_priority(page)
            rel_path = page.relative_to(self.root_dir)
            url_path = str(rel_path).replace('\\', '/')
            report += f"{i}. `{url_path}` - ä¼˜å…ˆçº§: {priority}\n"
        
        report += """
---

## ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ

### ç«‹å³æ‰§è¡Œ:

1. âœ… **éªŒè¯Sitemap**
   - è®¿é—®: https://www.xml-sitemaps.com/validate-xml-sitemap.html
   - è¾“å…¥: {base_url}/sitemap.xml
   - ç¡®è®¤æ— é”™è¯¯

2. âœ… **æäº¤åˆ°Google Search Console**
   - ç™»å½•: https://search.google.com/search-console
   - é€‰æ‹©å±æ€§: vaultcaddy.com
   - å·¦ä¾§èœå• â†’ "Sitemaps"
   - æ·»åŠ Sitemap URL: https://vaultcaddy.com/sitemap.xml
   - ç‚¹å‡»"æäº¤"

3. âœ… **æäº¤åˆ°Bing Webmaster Tools**
   - ç™»å½•: https://www.bing.com/webmasters
   - é€‰æ‹©ç½‘ç«™: vaultcaddy.com
   - é…ç½® â†’ Sitemaps
   - æäº¤: https://vaultcaddy.com/sitemap.xml

4. âœ… **æµ‹è¯•robots.txt**
   - è®¿é—®: {base_url}/robots.txt
   - ä½¿ç”¨Google Robots Testing TooléªŒè¯

### æœ¬å‘¨æ‰§è¡Œ:

5. âœ… **ç›‘æ§ç´¢å¼•çŠ¶æ€**
   - Google Search Console â†’ "è¦†ç›–ç‡"æŠ¥å‘Š
   - æŸ¥çœ‹å·²ç´¢å¼•é¡µé¢æ•°é‡
   - æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æˆ–è­¦å‘Š

6. âœ… **è®¾ç½®å®šæœŸæ›´æ–°**
   - æ¯å‘¨è¿è¡Œä¸€æ¬¡æ­¤è„šæœ¬
   - æˆ–è®¾ç½®è‡ªåŠ¨åŒ–ä»»åŠ¡ (cron job)

---

## ğŸ”§ è‡ªåŠ¨åŒ–æ›´æ–°

### æ–¹æ³•1: Cron Job (Linux/Mac)

```bash
# æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹æ›´æ–°Sitemap
0 3 * * 0 cd /Users/cavlinyeung/ai-bank-parser && python3 generate_sitemap.py
```

### æ–¹æ³•2: GitHub Actions (æ¨è)

åˆ›å»º `.github/workflows/update-sitemap.yml`:

```yaml
name: Update Sitemap
on:
  schedule:
    - cron: '0 3 * * 0'  # æ¯å‘¨æ—¥
  workflow_dispatch:  # æ‰‹åŠ¨è§¦å‘

jobs:
  update-sitemap:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      - name: Generate Sitemap
        run: python3 generate_sitemap.py
      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add sitemap.xml robots.txt
          git commit -m "ğŸ¤– Auto-update Sitemap"
          git push
```

---

## ğŸ‰ å®Œæˆæ¸…å•

- [ ] Sitemapå·²ç”Ÿæˆå¹¶éªŒè¯æ— é”™è¯¯
- [ ] robots.txtå·²ç”Ÿæˆ
- [ ] å·²æäº¤åˆ°Google Search Console
- [ ] å·²æäº¤åˆ°Bing Webmaster Tools
- [ ] GSCæ˜¾ç¤ºSitemapå·²æˆåŠŸå¤„ç†
- [ ] è®¾ç½®äº†è‡ªåŠ¨æ›´æ–°æœºåˆ¶
- [ ] ç›‘æ§ç´¢å¼•çŠ¶æ€æ­£å¸¸

---

**Sitemap URL**: `{base_url}/sitemap.xml`

**Robots.txt URL**: `{base_url}/robots.txt`

**æ‰€æœ‰4ä¸ªSEOé˜¶æ®µå·²å®Œæˆï¼** ğŸ‰

**é¢„æœŸæ•ˆæœ** (1-2æœˆ):
- ğŸ“ˆ é¡µé¢ç´¢å¼•ç‡ +50%
- ğŸ” è‡ªç„¶æµé‡ +40%
- ğŸ“Š å¹³å‡æ’åæå‡ 3-5ä½
- ğŸ’° è‡ªç„¶æ³¨å†Œ +40%
""".format(base_url=self.base_url)
        
        report_file = self.root_dir / 'âœ…_Sitemapç”Ÿæˆå®Œæˆ_Phase4.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    root_dir = '/Users/cavlinyeung/ai-bank-parser'
    base_url = 'https://vaultcaddy.com'
    
    generator = SitemapGenerator(root_dir, base_url)
    generator.collect_pages()
    generator.generate_sitemap()
    generator.generate_robots_txt()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰Sitemapå’Œrobots.txtå·²ç”Ÿæˆï¼")
    print("=" * 60)

if __name__ == '__main__':
    main()

