#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢SEOå®¡è®¡ - æ£€æŸ¥ç¼ºå¤±çš„å†…å®¹å’ŒSEOä¼˜åŒ–æœºä¼š
"""

import os
from pathlib import Path
import json

def seo_audit():
    """è¿›è¡Œå…¨é¢SEOå®¡è®¡"""
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘              ğŸ” VaultCaddy å…¨é¢SEOå®¡è®¡                                â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    issues = []
    recommendations = []
    
    # 1. æ£€æŸ¥æ”¶æ®ç›¸å…³å†…å®¹
    print("\n1ï¸âƒ£  æ£€æŸ¥æ”¶æ®ç›¸å…³å†…å®¹")
    print("="*70)
    
    receipt_files = {
        'zh': 'receipt-scanner.html',
        'en': 'en/receipt-scanner.html',
        'jp': 'jp/receipt-scanner.html',
        'kr': 'kr/receipt-scanner.html'
    }
    
    missing_receipt = []
    for lang, file in receipt_files.items():
        if os.path.exists(file):
            print(f"   âœ… {lang.upper()}: {file}")
        else:
            print(f"   âŒ {lang.upper()}: {file} - ç¼ºå¤±")
            missing_receipt.append((lang, file))
    
    if missing_receipt:
        issues.append({
            'category': 'æ”¶æ®é¡µé¢',
            'severity': 'HIGH',
            'issue': f'ç¼ºå°‘ {len(missing_receipt)} ä¸ªè¯­è¨€ç‰ˆæœ¬çš„æ”¶æ®æ‰«æé¡µé¢',
            'missing': missing_receipt
        })
    
    # 2. æ£€æŸ¥æ”¶æ®åšå®¢æ–‡ç« 
    print("\n2ï¸âƒ£  æ£€æŸ¥æ”¶æ®åšå®¢æ–‡ç« ")
    print("="*70)
    
    receipt_blog_topics = [
        'receipt-scanning-guide',
        'receipt-management-best-practices',
        'expense-tracking-with-receipts',
        'receipt-ocr-technology',
        'digital-receipt-management'
    ]
    
    for lang in ['en', 'jp', 'kr']:
        blog_dir = Path(f'{lang}/blog')
        if blog_dir.exists():
            existing = list(blog_dir.glob('*receipt*.html'))
            print(f"   {lang.upper()}: {len(existing)} ä¸ªæ”¶æ®ç›¸å…³æ–‡ç« ")
            if len(existing) == 0:
                issues.append({
                    'category': 'æ”¶æ®åšå®¢',
                    'severity': 'MEDIUM',
                    'issue': f'{lang.upper()} ç¼ºå°‘æ”¶æ®ç›¸å…³åšå®¢æ–‡ç« ',
                    'recommendation': 'åˆ›å»ºè‡³å°‘2-3ç¯‡æ”¶æ®ç›¸å…³æ–‡ç« '
                })
    
    # 3. æ£€æŸ¥Landing Pages
    print("\n3ï¸âƒ£  æ£€æŸ¥æ”¶æ®ç›¸å…³Landing Pages")
    print("="*70)
    
    for lang in ['en', 'jp', 'kr']:
        solutions_dir = Path(f'{lang}/solutions')
        if solutions_dir.exists():
            receipt_pages = list(solutions_dir.glob('*receipt*'))
            print(f"   {lang.upper()}: {len(receipt_pages)} ä¸ªæ”¶æ®ç›¸å…³landing page")
    
    # 4. æ£€æŸ¥robots.txt
    print("\n4ï¸âƒ£  æ£€æŸ¥robots.txt")
    print("="*70)
    
    if os.path.exists('robots.txt'):
        with open('robots.txt', 'r') as f:
            content = f.read()
            print(f"   âœ… robots.txt å­˜åœ¨")
            if 'Sitemap:' in content:
                print(f"   âœ… åŒ…å«Sitemapå¼•ç”¨")
            else:
                issues.append({
                    'category': 'robots.txt',
                    'severity': 'MEDIUM',
                    'issue': 'robots.txtç¼ºå°‘Sitemapå¼•ç”¨'
                })
    else:
        issues.append({
            'category': 'robots.txt',
            'severity': 'HIGH',
            'issue': 'robots.txtæ–‡ä»¶ä¸å­˜åœ¨'
        })
    
    # 5. æ£€æŸ¥Open Graphå›¾ç‰‡
    print("\n5ï¸âƒ£  æ£€æŸ¥Open Graphå›¾ç‰‡")
    print("="*70)
    
    og_images = Path('images').glob('og-*.png') if Path('images').exists() else []
    og_images = list(og_images)
    print(f"   æ‰¾åˆ° {len(og_images)} ä¸ªOGå›¾ç‰‡")
    
    if len(og_images) == 0:
        recommendations.append({
            'category': 'Open Graph',
            'priority': 'MEDIUM',
            'recommendation': 'åˆ›å»ºOpen Graphå›¾ç‰‡ï¼ˆ1200x630ï¼‰ç”¨äºç¤¾äº¤åª’ä½“åˆ†äº«'
        })
    
    # 6. æ£€æŸ¥Schema.orgæ ‡è®°
    print("\n6ï¸âƒ£  æ£€æŸ¥Schema.orgç»“æ„åŒ–æ•°æ®")
    print("="*70)
    
    pages_to_check = [
        'index.html',
        'en/index.html',
        'jp/index.html',
        'kr/index.html'
    ]
    
    missing_schema = []
    for page in pages_to_check:
        if os.path.exists(page):
            with open(page, 'r', encoding='utf-8') as f:
                content = f.read()
                if 'application/ld+json' in content:
                    print(f"   âœ… {page}: æœ‰Schema")
                else:
                    print(f"   âŒ {page}: ç¼ºå°‘Schema")
                    missing_schema.append(page)
    
    if missing_schema:
        issues.append({
            'category': 'Schema.org',
            'severity': 'MEDIUM',
            'issue': f'{len(missing_schema)} ä¸ªä¸»é¡µç¼ºå°‘ç»“æ„åŒ–æ•°æ®',
            'pages': missing_schema
        })
    
    # 7. æ£€æŸ¥å†…éƒ¨é“¾æ¥
    print("\n7ï¸âƒ£  æ£€æŸ¥å†…éƒ¨é“¾æ¥ä¼˜åŒ–")
    print("="*70)
    
    recommendations.append({
        'category': 'å†…éƒ¨é“¾æ¥',
        'priority': 'HIGH',
        'recommendation': 'åœ¨åšå®¢æ–‡ç« ä¹‹é—´æ·»åŠ ç›¸å…³æ–‡ç« é“¾æ¥'
    })
    
    recommendations.append({
        'category': 'å†…éƒ¨é“¾æ¥',
        'priority': 'MEDIUM',
        'recommendation': 'åœ¨Landing Pagesæ·»åŠ æŒ‡å‘åšå®¢æ–‡ç« çš„é“¾æ¥'
    })
    
    # 8. æ£€æŸ¥H1æ ‡ç­¾
    print("\n8ï¸âƒ£  æ£€æŸ¥H1æ ‡ç­¾ä¼˜åŒ–")
    print("="*70)
    
    recommendations.append({
        'category': 'H1æ ‡ç­¾',
        'priority': 'LOW',
        'recommendation': 'ç¡®ä¿æ¯ä¸ªé¡µé¢åªæœ‰ä¸€ä¸ªH1æ ‡ç­¾ï¼ŒåŒ…å«ä¸»è¦å…³é”®è¯'
    })
    
    # 9. æ£€æŸ¥ç§»åŠ¨ç«¯å‹å¥½æ€§
    print("\n9ï¸âƒ£  ç§»åŠ¨ç«¯å‹å¥½æ€§")
    print("="*70)
    
    recommendations.append({
        'category': 'ç§»åŠ¨ç«¯',
        'priority': 'MEDIUM',
        'recommendation': 'åœ¨Google Search Consoleä¸­æ£€æŸ¥ç§»åŠ¨ç«¯å¯ç”¨æ€§'
    })
    
    # 10. æ£€æŸ¥é¡µé¢åŠ è½½é€Ÿåº¦
    print("\nğŸ”Ÿ é¡µé¢åŠ è½½é€Ÿåº¦")
    print("="*70)
    
    recommendations.append({
        'category': 'é¡µé¢é€Ÿåº¦',
        'priority': 'HIGH',
        'recommendation': 'ä½¿ç”¨Google PageSpeed Insightsæ£€æŸ¥æ‰€æœ‰ä¸»è¦é¡µé¢'
    })
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š å®¡è®¡ç»“æœæ€»ç»“")
    print("="*70)
    
    print(f"\nğŸš¨ å‘ç°é—®é¢˜ï¼š{len(issues)} ä¸ª")
    for i, issue in enumerate(issues, 1):
        print(f"\n   {i}. [{issue['severity']}] {issue['category']}")
        print(f"      é—®é¢˜ï¼š{issue['issue']}")
        if 'recommendation' in issue:
            print(f"      å»ºè®®ï¼š{issue['recommendation']}")
    
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®ï¼š{len(recommendations)} ä¸ª")
    for i, rec in enumerate(recommendations, 1):
        print(f"\n   {i}. [{rec['priority']}] {rec['category']}")
        print(f"      å»ºè®®ï¼š{rec['recommendation']}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report = {
        'issues': issues,
        'recommendations': recommendations,
        'summary': {
            'total_issues': len(issues),
            'high_severity': len([i for i in issues if i['severity'] == 'HIGH']),
            'medium_severity': len([i for i in issues if i['severity'] == 'MEDIUM']),
            'total_recommendations': len(recommendations)
        }
    }
    
    with open('seo_audit_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼šseo_audit_report.json")
    
    return issues, recommendations

if __name__ == '__main__':
    seo_audit()

