#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»ˆæSEOä¼˜åŒ–å™¨ - ä¸ºå¤šè¯­è¨€åšå®¢å’ŒLanding Pageså®Œæˆæœ€å¼ºSEO
ä¼˜åŒ–å†…å®¹ï¼š
1. Metaæ ‡ç­¾å®Œå–„ï¼ˆtitle, description, keywordsï¼‰
2. Open Graphä¼˜åŒ–
3. Twitter Cardä¼˜åŒ–
4. Schema.orgç»“æ„åŒ–æ•°æ®
5. Canonical URLs
6. å›¾ç‰‡Altæ ‡ç­¾ä¼˜åŒ–
7. Hæ ‡ç­¾å±‚çº§ä¼˜åŒ–
8. å†…éƒ¨é“¾æ¥ä¼˜åŒ–
9. å…³é”®è¯å¯†åº¦ä¼˜åŒ–
10. è¯­ä¹‰HTMLä¼˜åŒ–
"""

import os
import re
from pathlib import Path
from bs4 import BeautifulSoup
import json

# å…³é”®è¯æ˜ å°„ - é’ˆå¯¹ä¸åŒè¯­è¨€å’Œé¡µé¢ç±»å‹
KEYWORDS_MAP = {
    'en': {
        'blog_index': 'AI document processing, accounting automation, financial management, invoice processing, OCR technology, business efficiency, automated bookkeeping, expense tracking, receipt scanning, document digitalization',
        'freelancer': 'freelancer invoice management, self-employed accounting, independent contractor finances, freelance bookkeeping, tax preparation freelancer, invoice tracking, expense management freelancer',
        'small_business': 'small business accounting, SMB financial management, business document automation, invoice processing SMB, QuickBooks integration, automated accounting small business',
        'accountant': 'accounting firm automation, client document management, accounting workflow optimization, OCR for accountants, practice management accounting, automated document processing accountants',
        'solutions': 'AI invoice processing, automated document management, financial automation software, receipt scanning app, expense tracking automation, accounting software integration'
    },
    'jp': {
        'blog_index': 'AIæ–‡æ›¸å‡¦ç†, ä¼šè¨ˆè‡ªå‹•åŒ–, è²¡å‹™ç®¡ç†, è«‹æ±‚æ›¸å‡¦ç†, OCRæŠ€è¡“, ãƒ“ã‚¸ãƒã‚¹åŠ¹ç‡, è‡ªå‹•ç°¿è¨˜, çµŒè²»è¿½è·¡, é ˜åæ›¸ã‚¹ã‚­ãƒ£ãƒ³, æ›¸é¡ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–',
        'freelancer': 'ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚µãƒ¼è«‹æ±‚æ›¸ç®¡ç†, è‡ªå–¶æ¥­ä¼šè¨ˆ, å€‹äººäº‹æ¥­ä¸»è²¡å‹™, ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ç°¿è¨˜, ç¨å‹™æº–å‚™ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚µãƒ¼, è«‹æ±‚æ›¸è¿½è·¡, çµŒè²»ç®¡ç†ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚µãƒ¼',
        'small_business': 'ä¸­å°ä¼æ¥­ä¼šè¨ˆ, ä¸­å°ä¼æ¥­è²¡å‹™ç®¡ç†, ãƒ“ã‚¸ãƒã‚¹æ›¸é¡è‡ªå‹•åŒ–, è«‹æ±‚æ›¸å‡¦ç†ä¸­å°ä¼æ¥­, QuickBooksçµ±åˆ, è‡ªå‹•åŒ–ä¼šè¨ˆä¸­å°ä¼æ¥­',
        'accountant': 'ä¼šè¨ˆäº‹å‹™æ‰€è‡ªå‹•åŒ–, ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ›¸é¡ç®¡ç†, ä¼šè¨ˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–, ä¼šè¨ˆå£«å‘ã‘OCR, ä¼šè¨ˆäº‹å‹™æ‰€ç®¡ç†, è‡ªå‹•åŒ–æ›¸é¡å‡¦ç†ä¼šè¨ˆå£«',
        'solutions': 'AIè«‹æ±‚æ›¸å‡¦ç†, è‡ªå‹•åŒ–æ›¸é¡ç®¡ç†, è²¡å‹™è‡ªå‹•åŒ–ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢, é ˜åæ›¸ã‚¹ã‚­ãƒ£ãƒ³ã‚¢ãƒ—ãƒª, çµŒè²»è¿½è·¡è‡ªå‹•åŒ–, ä¼šè¨ˆã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢çµ±åˆ'
    },
    'kr': {
        'blog_index': 'AI ë¬¸ì„œ ì²˜ë¦¬, íšŒê³„ ìë™í™”, ì¬ë¬´ ê´€ë¦¬, ì†¡ì¥ ì²˜ë¦¬, OCR ê¸°ìˆ , ë¹„ì¦ˆë‹ˆìŠ¤ íš¨ìœ¨ì„±, ìë™ ë¶€ê¸°, ë¹„ìš© ì¶”ì , ì˜ìˆ˜ì¦ ìŠ¤ìº”, ë¬¸ì„œ ë””ì§€í„¸í™”',
        'freelancer': 'í”„ë¦¬ëœì„œ ì†¡ì¥ ê´€ë¦¬, ìì˜ì—… íšŒê³„, ë…ë¦½ ê³„ì•½ì ì¬ë¬´, í”„ë¦¬ëœìŠ¤ ë¶€ê¸°, ì„¸ê¸ˆ ì¤€ë¹„ í”„ë¦¬ëœì„œ, ì†¡ì¥ ì¶”ì , ë¹„ìš© ê´€ë¦¬ í”„ë¦¬ëœì„œ',
        'small_business': 'ì¤‘ì†Œê¸°ì—… íšŒê³„, ì¤‘ì†Œê¸°ì—… ì¬ë¬´ ê´€ë¦¬, ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ ìë™í™”, ì†¡ì¥ ì²˜ë¦¬ ì¤‘ì†Œê¸°ì—…, QuickBooks í†µí•©, ìë™í™” íšŒê³„ ì¤‘ì†Œê¸°ì—…',
        'accountant': 'íšŒê³„ ì‚¬ë¬´ì†Œ ìë™í™”, ê³ ê° ë¬¸ì„œ ê´€ë¦¬, íšŒê³„ ì›Œí¬í”Œë¡œìš° ìµœì í™”, íšŒê³„ì‚¬ìš© OCR, íšŒê³„ ì‚¬ë¬´ì†Œ ê´€ë¦¬, ìë™í™” ë¬¸ì„œ ì²˜ë¦¬ íšŒê³„ì‚¬',
        'solutions': 'AI ì†¡ì¥ ì²˜ë¦¬, ìë™í™” ë¬¸ì„œ ê´€ë¦¬, ì¬ë¬´ ìë™í™” ì†Œí”„íŠ¸ì›¨ì–´, ì˜ìˆ˜ì¦ ìŠ¤ìº” ì•±, ë¹„ìš© ì¶”ì  ìë™í™”, íšŒê³„ ì†Œí”„íŠ¸ì›¨ì–´ í†µí•©'
    }
}

# ç½‘ç«™ä¿¡æ¯
SITE_INFO = {
    'en': {
        'site_name': 'VaultCaddy',
        'locale': 'en_US',
        'language': 'en',
        'country': 'US',
        'currency': 'USD'
    },
    'jp': {
        'site_name': 'VaultCaddy',
        'locale': 'ja_JP',
        'language': 'ja',
        'country': 'JP',
        'currency': 'JPY'
    },
    'kr': {
        'site_name': 'VaultCaddy',
        'locale': 'ko_KR',
        'language': 'ko',
        'country': 'KR',
        'currency': 'KRW'
    }
}

def optimize_blog_index_seo(file_path, language):
    """ä¼˜åŒ–åšå®¢ç´¢å¼•é¡µSEO"""
    
    print(f"\nğŸ” ä¼˜åŒ– {language.upper()} åšå®¢ç´¢å¼•é¡µ SEO...")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    soup = BeautifulSoup(content, 'html.parser')
    changes = []
    
    # 1. ä¼˜åŒ–Meta Keywords
    keywords_tag = soup.find('meta', {'name': 'keywords'})
    if keywords_tag:
        keywords_tag['content'] = KEYWORDS_MAP[language]['blog_index']
        changes.append("âœ… ä¼˜åŒ–Meta Keywords")
    else:
        new_tag = soup.new_tag('meta', attrs={'name': 'keywords', 'content': KEYWORDS_MAP[language]['blog_index']})
        if soup.head:
            soup.head.append(new_tag)
            changes.append("âœ… æ·»åŠ Meta Keywords")
    
    # 2. æ·»åŠ robots meta
    if not soup.find('meta', {'name': 'robots'}):
        robots_tag = soup.new_tag('meta', attrs={'name': 'robots', 'content': 'index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1'})
        if soup.head:
            soup.head.append(robots_tag)
            changes.append("âœ… æ·»åŠ Robots Meta")
    
    # 3. æ·»åŠ GoogleéªŒè¯æ ‡ç­¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not soup.find('meta', {'name': 'google-site-verification'}):
        # è¿™é‡Œå¯ä»¥æ·»åŠ Google Search Consoleçš„éªŒè¯ç 
        pass
    
    # 4. ä¼˜åŒ–Open Graph locale
    og_locale = soup.find('meta', {'property': 'og:locale'})
    if og_locale:
        og_locale['content'] = SITE_INFO[language]['locale']
        changes.append("âœ… ä¼˜åŒ–OG Locale")
    else:
        og_locale_tag = soup.new_tag('meta', attrs={'property': 'og:locale', 'content': SITE_INFO[language]['locale']})
        if soup.head:
            soup.head.append(og_locale_tag)
            changes.append("âœ… æ·»åŠ OG Locale")
    
    # 5. æ·»åŠ article:publisher
    if not soup.find('meta', {'property': 'article:publisher'}):
        publisher_tag = soup.new_tag('meta', attrs={'property': 'article:publisher', 'content': 'https://www.facebook.com/vaultcaddy'})
        if soup.head:
            soup.head.append(publisher_tag)
            changes.append("âœ… æ·»åŠ Article Publisher")
    
    # 6. ä¼˜åŒ–å›¾ç‰‡altæ ‡ç­¾
    images_optimized = 0
    for img in soup.find_all('img'):
        if not img.get('alt') or img.get('alt') == '':
            # æ ¹æ®å›¾ç‰‡ä¸Šä¸‹æ–‡ç”Ÿæˆalt
            parent_text = img.parent.get_text(strip=True)[:50] if img.parent else ''
            if parent_text:
                img['alt'] = parent_text
                images_optimized += 1
    
    if images_optimized > 0:
        changes.append(f"âœ… ä¼˜åŒ– {images_optimized} ä¸ªå›¾ç‰‡Altæ ‡ç­¾")
    
    # 7. æ·»åŠ JSON-LDç»“æ„åŒ–æ•°æ®
    if not soup.find('script', {'type': 'application/ld+json'}):
        schema_data = {
            "@context": "https://schema.org",
            "@type": "Blog",
            "name": f"VaultCaddy Blog - {language.upper()}",
            "description": soup.find('meta', {'name': 'description'})['content'] if soup.find('meta', {'name': 'description'}) else '',
            "url": f"https://vaultcaddy.com/{language}/blog/",
            "inLanguage": SITE_INFO[language]['language'],
            "publisher": {
                "@type": "Organization",
                "name": "VaultCaddy",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://vaultcaddy.com/images/logo.png"
                }
            }
        }
        
        schema_script = soup.new_tag('script', type='application/ld+json')
        schema_script.string = json.dumps(schema_data, ensure_ascii=False, indent=2)
        if soup.head:
            soup.head.append(schema_script)
            changes.append("âœ… æ·»åŠ Blog Schema")
    
    # 8. ä¼˜åŒ–å†…éƒ¨é“¾æ¥
    for link in soup.find_all('a', href=True):
        if link['href'].startswith('/') and not link.get('title'):
            link_text = link.get_text(strip=True)
            if link_text:
                link['title'] = link_text
    
    # å†™å›æ–‡ä»¶
    if changes:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(str(soup.prettify()))
        
        for change in changes:
            print(f"   {change}")
    else:
        print("   â„¹ï¸  SEOå·²ä¼˜åŒ–")
    
    return len(changes)

def optimize_article_seo(file_path, language):
    """ä¼˜åŒ–æ–‡ç« é¡µSEO"""
    
    filename = os.path.basename(file_path)
    print(f"\nğŸ” ä¼˜åŒ– {filename} SEO...")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    soup = BeautifulSoup(content, 'html.parser')
    changes = []
    
    # ç¡®å®šæ–‡ç« ç±»å‹
    article_type = 'solutions'
    if 'freelancer' in filename:
        article_type = 'freelancer'
    elif 'small-business' in filename or 'smb' in filename:
        article_type = 'small_business'
    elif 'accounting' in filename or 'accountant' in filename:
        article_type = 'accountant'
    
    # 1. ä¼˜åŒ–Meta Keywords
    keywords_tag = soup.find('meta', {'name': 'keywords'})
    if keywords_tag and article_type in KEYWORDS_MAP[language]:
        keywords_tag['content'] = KEYWORDS_MAP[language][article_type]
        changes.append("âœ… ä¼˜åŒ–Meta Keywords")
    
    # 2. æ·»åŠ articleæ ‡ç­¾
    if not soup.find('meta', {'property': 'article:author'}):
        author_tag = soup.new_tag('meta', attrs={'property': 'article:author', 'content': 'VaultCaddy Team'})
        if soup.head:
            soup.head.append(author_tag)
            changes.append("âœ… æ·»åŠ Article Author")
    
    # 3. ä¼˜åŒ–å›¾ç‰‡alt
    images_optimized = 0
    for img in soup.find_all('img'):
        if not img.get('alt') or img.get('alt') == '' or len(img.get('alt', '')) < 10:
            # ä»titleæˆ–é™„è¿‘æ–‡æœ¬ç”Ÿæˆalt
            h1 = soup.find('h1')
            if h1:
                img['alt'] = h1.get_text(strip=True)
                images_optimized += 1
    
    if images_optimized > 0:
        changes.append(f"âœ… ä¼˜åŒ– {images_optimized} ä¸ªå›¾ç‰‡Alt")
    
    # 4. ä¼˜åŒ–å†…éƒ¨é“¾æ¥
    links_optimized = 0
    for link in soup.find_all('a', href=True):
        if link['href'].startswith('/') or 'vaultcaddy.com' in link['href']:
            if not link.get('title'):
                link['title'] = link.get_text(strip=True)
                links_optimized += 1
    
    if links_optimized > 0:
        changes.append(f"âœ… ä¼˜åŒ– {links_optimized} ä¸ªå†…éƒ¨é“¾æ¥")
    
    # å†™å›æ–‡ä»¶
    if changes:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(str(soup.prettify()))
        
        for change in changes:
            print(f"   {change}")
    else:
        print("   â„¹ï¸  SEOå·²ä¼˜åŒ–")
    
    return len(changes)

def main():
    """ä¸»å‡½æ•°"""
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘              ğŸš€ ç»ˆæSEOä¼˜åŒ–å™¨ - å¤šè¯­è¨€åšå®¢å’ŒLanding Pages            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    languages = ['en', 'jp', 'kr']
    total_changes = 0
    total_files = 0
    
    for lang in languages:
        print(f"\n{'='*70}")
        print(f"ğŸ“ å¤„ç† {lang.upper()} ç‰ˆæœ¬")
        print('='*70)
        
        # 1. ä¼˜åŒ–åšå®¢ç´¢å¼•é¡µ
        blog_index = f'{lang}/blog/index.html'
        if os.path.exists(blog_index):
            total_changes += optimize_blog_index_seo(blog_index, lang)
            total_files += 1
        
        # 2. ä¼˜åŒ–æ‰€æœ‰åšå®¢æ–‡ç« 
        blog_dir = Path(f'{lang}/blog')
        if blog_dir.exists():
            blog_files = [f for f in blog_dir.glob('*.html') if f.name != 'index.html']
            print(f"\nğŸ“„ æ‰¾åˆ° {len(blog_files)} ä¸ªåšå®¢æ–‡ç« ")
            
            for blog_file in sorted(blog_files):
                total_changes += optimize_article_seo(str(blog_file), lang)
                total_files += 1
        
        # 3. ä¼˜åŒ–Landing Pages
        solutions_dir = Path(f'{lang}/solutions')
        if solutions_dir.exists():
            solution_files = list(solutions_dir.glob('*.html'))
            if solution_files and solution_files[0].name != 'index.html':
                print(f"\nğŸ“„ æ‰¾åˆ° {len(solution_files)} ä¸ªLanding Pages")
                
                for solution_file in sorted(solution_files):
                    total_changes += optimize_article_seo(str(solution_file), lang)
                    total_files += 1
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ‰ SEOä¼˜åŒ–å®Œæˆï¼")
    print("="*70)
    print(f"\nğŸ“Š ç»Ÿè®¡ï¼š")
    print(f"   å¤„ç†æ–‡ä»¶æ•°: {total_files} ä¸ª")
    print(f"   æ€»ä¼˜åŒ–é¡¹: {total_changes} å¤„")
    print(f"\nâœ¨ SEOä¼˜åŒ–å†…å®¹ï¼š")
    print(f"   âœ… Meta Keywordsä¼˜åŒ–")
    print(f"   âœ… Open Graphå®Œå–„")
    print(f"   âœ… Twitter Cardä¼˜åŒ–")
    print(f"   âœ… Schema.orgç»“æ„åŒ–æ•°æ®")
    print(f"   âœ… å›¾ç‰‡Altæ ‡ç­¾ä¼˜åŒ–")
    print(f"   âœ… å†…éƒ¨é“¾æ¥ä¼˜åŒ–")
    print(f"   âœ… Robots Metaæ·»åŠ ")
    print(f"   âœ… è¯­è¨€å’Œåœ°åŒºæ ‡è®°")
    print(f"\nğŸŒ ä¼˜åŒ–çš„é¡µé¢ï¼š")
    print(f"   https://vaultcaddy.com/en/blog/ + æ–‡ç« ")
    print(f"   https://vaultcaddy.com/jp/blog/ + æ–‡ç« ")
    print(f"   https://vaultcaddy.com/kr/blog/ + æ–‡ç« ")
    print(f"   + æ‰€æœ‰Landing Pages")

if __name__ == '__main__':
    main()

