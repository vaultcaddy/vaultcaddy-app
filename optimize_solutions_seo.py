#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–Solutions Landing Pagesçš„SEO
"""

import os
import re
from pathlib import Path
from bs4 import BeautifulSoup
import json

def optimize_solution_page(file_path, language):
    """ä¼˜åŒ–å•ä¸ªsolution pageçš„SEO"""
    
    filename = os.path.basename(file_path)
    print(f"ğŸ” {filename}...", end='')
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        soup = BeautifulSoup(content, 'html.parser')
        changes = 0
        
        # 1. æ·»åŠ robots meta
        if not soup.find('meta', {'name': 'robots'}):
            robots_tag = soup.new_tag('meta', attrs={'name': 'robots', 'content': 'index, follow'})
            if soup.head:
                soup.head.insert(0, robots_tag)
                changes += 1
        
        # 2. ä¼˜åŒ–å›¾ç‰‡alt
        for img in soup.find_all('img'):
            if not img.get('alt') or len(img.get('alt', '')) < 5:
                h1 = soup.find('h1')
                if h1:
                    img['alt'] = h1.get_text(strip=True)[:100]
                    changes += 1
        
        # 3. æ·»åŠ internal linksçš„title
        for link in soup.find_all('a', href=True):
            if link['href'].startswith('/') or 'vaultcaddy.com' in link['href']:
                if not link.get('title'):
                    link_text = link.get_text(strip=True)
                    if link_text:
                        link['title'] = link_text
                        changes += 1
        
        # 4. ç¡®ä¿æœ‰canonical URL
        if not soup.find('link', {'rel': 'canonical'}):
            # ä»meta og:urlè·å–
            og_url = soup.find('meta', {'property': 'og:url'})
            if og_url and og_url.get('content'):
                canonical_tag = soup.new_tag('link', attrs={'rel': 'canonical', 'href': og_url['content']})
                if soup.head:
                    soup.head.append(canonical_tag)
                    changes += 1
        
        # å†™å›æ–‡ä»¶
        if changes > 0:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(str(soup.prettify()))
            print(f" âœ… {changes}å¤„")
        else:
            print(" âœ“")
        
        return changes
        
    except Exception as e:
        print(f" âŒ é”™è¯¯: {str(e)}")
        return 0

def main():
    """ä¸»å‡½æ•°"""
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘              ğŸš€ Solutions Landing Pages SEOä¼˜åŒ–                       â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    languages = ['en', 'jp', 'kr']
    total_changes = 0
    total_files = 0
    
    for lang in languages:
        print(f"\n{'='*70}")
        print(f"ğŸ“ å¤„ç† {lang.upper()} Solutions")
        print('='*70)
        
        solutions_dir = Path(f'{lang}/solutions')
        if not solutions_dir.exists():
            print(f"   âš ï¸  ç›®å½•ä¸å­˜åœ¨: {solutions_dir}")
            continue
        
        # è·å–æ‰€æœ‰HTMLæ–‡ä»¶ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰
        html_files = list(solutions_dir.rglob('*.html'))
        print(f"\næ‰¾åˆ° {len(html_files)} ä¸ªLanding Pages")
        print()
        
        for html_file in sorted(html_files):
            changes = optimize_solution_page(str(html_file), lang)
            total_changes += changes
            total_files += 1
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ‰ Solutions SEOä¼˜åŒ–å®Œæˆï¼")
    print("="*70)
    print(f"\nğŸ“Š ç»Ÿè®¡ï¼š")
    print(f"   å¤„ç†æ–‡ä»¶æ•°: {total_files} ä¸ª")
    print(f"   æ€»ä¼˜åŒ–é¡¹: {total_changes} å¤„")
    print(f"\nâœ¨ ä¼˜åŒ–å†…å®¹ï¼š")
    print(f"   âœ… Robots Metaæ ‡ç­¾")
    print(f"   âœ… å›¾ç‰‡Altæ ‡ç­¾")
    print(f"   âœ… å†…éƒ¨é“¾æ¥Title")
    print(f"   âœ… Canonical URLs")

if __name__ == '__main__':
    main()

